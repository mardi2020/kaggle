{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-23T12:42:18.803802Z",
     "start_time": "2025-02-23T12:42:18.801760Z"
    }
   },
   "outputs": [],
   "source": [
    "# csv, df\n",
    "CLEAN_TEXT = 'clean_text'\n",
    "TOKEN_IDS = 'token_ids'\n",
    "EMBEDDING = 'embedding'\n",
    "TARGET = 'target'\n",
    "ATTENTION_MASK = 'attention_mask'\n",
    "\n",
    "# training models\n",
    "MAX_EPOCH = 7\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-6\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "# models\n",
    "INPUT_DIM = 768\n",
    "HIDDEN_DIM = 256  # 256 ~ 512\n",
    "\n",
    "# BERT\n",
    "MAX_LENGTH = 128\n",
    "BERT_MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "# ÌååÏùº Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "TRAIN_PICKLE_PATH = \"twitter_disaster/data/output/train_bert_embeddings.pkl\"\n",
    "TEST_PICKLE_PATH = \"twitter_disaster/data/output/test_bert_embeddings.pkl\"\n",
    "MODEL_SAVE_PATH = \"twitter_disaster/models/classifier.pth\"\n",
    "SUBMISSION_SAVE_PATH = 'twitter_disaster/data/output/submission.csv'\n",
    "\n",
    "# MPS\n",
    "DEVICE = \"mps\" if __import__(\"torch\").backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import demoji\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T11:15:11.272165Z",
     "start_time": "2025-02-23T11:15:10.257683Z"
    }
   },
   "id": "74a7e551000a7726",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('twitter_disaster/data/input/train.csv',\n",
    "                           usecols=['text', 'target'],\n",
    "                           dtype={'text': str, 'target': np.int64})\n",
    "test_df = pd.read_csv('twitter_disaster/data/input/test.csv',\n",
    "                       usecols=['text', 'id'],\n",
    "                       dtype={'text': str, 'target': np.int64}\n",
    "                       )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T11:20:59.674415Z",
     "start_time": "2025-02-23T11:20:59.650615Z"
    }
   },
   "id": "be1eb632d010e4cf",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "target\n0    4342\n1    3271\nName: count, dtype: int64"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T11:17:56.709955Z",
     "start_time": "2025-02-23T11:17:56.707309Z"
    }
   },
   "id": "1be6f06d87c3bd15",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   id                                               text\n0   0                 Just happened a terrible car crash\n1   2  Heard about #earthquake is different cities, s...\n2   3  there is a forest fire at spot pond, geese are...\n3   9           Apocalypse lighting. #Spokane #wildfires\n4  11      Typhoon Soudelor kills 28 in China and Taiwan",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T11:16:40.559999Z",
     "start_time": "2025-02-23T11:16:40.557183Z"
    }
   },
   "id": "f84c1be964693b1",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "    if \"target\" in df.columns:\n",
    "        df = df.dropna(subset=[\"target\"])\n",
    "    df.loc[:, 'clean_text'] = df['text'].apply(cleaning_txt)\n",
    "    return df\n",
    "\n",
    "def cleaning_txt(text: str):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\n\", \" \", text)  # Í∞úÌñâ Ï†úÍ±∞\n",
    "    text = re.sub(r\"&amp;\", \"and\", text)  # HTML ÏóîÌã∞Ìã∞ Î≥ÄÌôò\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)  # URL Ï†úÍ±∞\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)  # Î©òÏÖò Ï†úÍ±∞\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Ïà´Ïûê Ï†úÍ±∞\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # ÌäπÏàò Î¨∏Ïûê Ï†úÍ±∞\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    text = demoji.replace(text, \"\")\n",
    "\n",
    "    words = text.split()\n",
    "    words = [\n",
    "        lemma.lemmatize(word) for word in words if word.lower() not in stop_words\n",
    "    ]\n",
    "    return \" \".join(words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T11:17:08.106603Z",
     "start_time": "2025-02-23T11:17:07.972300Z"
    }
   },
   "id": "5ca2b68c3396eb8b",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "indices = [4415, 4400, 4399, 4403, 4397, 4396, 4394, 4414, 4393, 4392,\n",
    "                 4404, 4407, 4420, 4412, 4408, 4391, 4405,\n",
    "                 6840, 6834, 6837, 6841, 6816, 6828, 6831,\n",
    "                 246, 270, 266, 259, 253, 251, 250, 271,\n",
    "                 6119, 6122, 6123, 6131, 6160, 6166, 6167, 6172, 6212, 6221, 6230, 6091, 6108,\n",
    "                 7435, 7460, 7464, 7466, 7469, 7475, 7489, 7495, 7500, 7525, 7552, 7572, 7591, 7599]\n",
    "train_df.loc[indices, 'target'] = 0\n",
    "indices = [3913, 3914, 3936, 3921, 3941, 3937, 3938, 3136, 3133, 3930, 3933, 3924, 3917]\n",
    "train_df.loc[indices, 'target'] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T11:21:03.068930Z",
     "start_time": "2025-02-23T11:21:03.064281Z"
    }
   },
   "id": "2409e0feb866e51b",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "target\n0    4370\n1    3243\nName: count, dtype: int64"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T11:21:05.033911Z",
     "start_time": "2025-02-23T11:21:05.029806Z"
    }
   },
   "id": "5bce9a7cc20f8793",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "\n",
    "def tokenize_and_convert(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        text = \"[PAD]\"\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"].squeeze(0).to(DEVICE, dtype=torch.long)\n",
    "    attention_mask = encoding[\"attention_mask\"].squeeze(0).to(DEVICE, dtype=torch.long)\n",
    "\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "\n",
    "def bert_embedding(input_ids, attention_mask):\n",
    "    if input_ids.dim() == 1:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "    if attention_mask.dim() == 1:\n",
    "        attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask)\n",
    "\n",
    "    return output.logits.squeeze(0).cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T11:19:59.568068Z",
     "start_time": "2025-02-23T11:19:57.287255Z"
    }
   },
   "id": "2fb36d5376aafa6b",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    token_ids, attention_mask = tokenize_and_convert(text)\n",
    "    embedding = bert_embedding(token_ids, attention_mask)\n",
    "    return token_ids.squeeze(0).cpu().numpy(), attention_mask.squeeze(0).cpu().numpy(), embedding\n",
    "\n",
    "\n",
    "def start_embedding(df, path=TRAIN_PICKLE_PATH):\n",
    "    df = preprocess(df)\n",
    "    results = [process_text(text) for text in df[CLEAN_TEXT].tolist()]\n",
    "    token_ids_list, attention_masks_list, embeddings_list = zip(*results)\n",
    "    df[TOKEN_IDS] = list(token_ids_list)\n",
    "    df[ATTENTION_MASK] = list(attention_masks_list)\n",
    "    df.to_pickle(path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T11:21:09.232788Z",
     "start_time": "2025-02-23T11:21:09.228297Z"
    }
   },
   "id": "e0fb3cda32a19640",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_df = start_embedding(train_df, TRAIN_PICKLE_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T12:20:21.437669Z",
     "start_time": "2025-02-23T12:17:48.805527Z"
    }
   },
   "id": "7fec75124da31d33",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_df = start_embedding(test_df, TEST_PICKLE_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T12:21:01.337296Z",
     "start_time": "2025-02-23T12:20:29.698830Z"
    }
   },
   "id": "e5635045120911b5",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                         attention_mask  \\\n0     [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n1     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...   \n2     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...   \n3     [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...   \n...                                                 ...   \n7608  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...   \n7609  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...   \n7610  [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n7611  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n7612  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...   \n\n                                              token_ids  target  \n0     [101, 15046, 3114, 8372, 2089, 16455, 9641, 10...       1  \n1     [101, 3224, 2543, 2379, 2474, 6902, 3351, 2187...       1  \n2     [101, 6319, 2356, 7713, 2173, 19488, 2961, 139...       1  \n3     [101, 2111, 4374, 3748, 10273, 13982, 2344, 26...       1  \n4     [101, 2288, 2741, 6302, 10090, 7397, 5610, 374...       1  \n...                                                 ...     ...  \n7608  [101, 2048, 5016, 11308, 3173, 2958, 7859, 351...       1  \n7609  [101, 2491, 3748, 2543, 2662, 2130, 2642, 2112...       1  \n7610  [101, 11396, 22287, 12779, 7359, 102, 0, 0, 0,...       1  \n7611  [101, 2610, 11538, 1041, 5638, 3489, 17745, 24...       1  \n7612  [101, 6745, 2188, 10958, 5422, 2642, 2662, 374...       1  \n\n[7613 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attention_mask</th>\n      <th>token_ids</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[101, 15046, 3114, 8372, 2089, 16455, 9641, 10...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...</td>\n      <td>[101, 3224, 2543, 2379, 2474, 6902, 3351, 2187...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n      <td>[101, 6319, 2356, 7713, 2173, 19488, 2961, 139...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[101, 2111, 4374, 3748, 10273, 13982, 2344, 26...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n      <td>[101, 2288, 2741, 6302, 10090, 7397, 5610, 374...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7608</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...</td>\n      <td>[101, 2048, 5016, 11308, 3173, 2958, 7859, 351...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7609</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n      <td>[101, 2491, 3748, 2543, 2662, 2130, 2642, 2112...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7610</th>\n      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[101, 11396, 22287, 12779, 7359, 102, 0, 0, 0,...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7611</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[101, 2610, 11538, 1041, 5638, 3489, 17745, 24...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7612</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n      <td>[101, 6745, 2188, 10958, 5422, 2642, 2662, 374...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>7613 rows √ó 3 columns</p>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['attention_mask', 'token_ids', 'target']\n",
    "train_df = pd.read_pickle(TRAIN_PICKLE_PATH)[columns]\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T12:22:01.053370Z",
     "start_time": "2025-02-23T12:22:01.029764Z"
    }
   },
   "id": "f3a2e616e6010354",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                      attention_mask  \\\n0  [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...   \n3  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n\n                                           token_ids  id  \n0  [101, 3047, 6659, 2482, 5823, 102, 0, 0, 0, 0,...   0  \n1  [101, 2657, 8372, 2367, 2103, 2994, 3647, 3071...   2  \n2  [101, 3224, 2543, 3962, 8644, 13020, 14070, 24...   3  \n3  [101, 16976, 7497, 21878, 3748, 10273, 102, 0,...   9  \n4  [101, 15393, 2061, 12672, 10626, 3102, 2859, 6...  11  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attention_mask</th>\n      <th>token_ids</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[101, 3047, 6659, 2482, 5823, 102, 0, 0, 0, 0,...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[101, 2657, 8372, 2367, 2103, 2994, 3647, 3071...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n      <td>[101, 3224, 2543, 3962, 8644, 13020, 14070, 24...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[101, 16976, 7497, 21878, 3748, 10273, 102, 0,...</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[101, 15393, 2061, 12672, 10626, 3102, 2859, 6...</td>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['attention_mask', 'token_ids', 'id']\n",
    "test_df = pd.read_pickle(TEST_PICKLE_PATH)[columns]\n",
    "test_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T12:22:53.480498Z",
     "start_time": "2025-02-23T12:22:53.467293Z"
    }
   },
   "id": "f098640ce24ca114",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "target\n0    4370\n1    3243\nName: count, dtype: int64"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[TARGET].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T12:22:53.483256Z",
     "start_time": "2025-02-23T12:22:53.481003Z"
    }
   },
   "id": "85b1d646420924ea",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel\n",
    "\n",
    "class DisasterDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = torch.tensor(np.vstack(embeddings), dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "class DisasterClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DisasterClassifier, self).__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(DEVICE)\n",
    "        self.fc1 = nn.Linear(INPUT_DIM, HIDDEN_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(HIDDEN_DIM)  # BatchNorm Ï∂îÍ∞Ä\n",
    "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "        self.fc2 = nn.Linear(HIDDEN_DIM, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_out = self.bert(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask\n",
    "                             ).last_hidden_state[:, 0, :]\n",
    "        x = self.fc1(bert_out)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T12:23:58.166406Z",
     "start_time": "2025-02-23T12:23:58.162313Z"
    }
   },
   "id": "a64d1008634455fc",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict()\n",
    "    }, MODEL_SAVE_PATH)\n",
    "\n",
    "\n",
    "\n",
    "def train_model(train_df):\n",
    "    core(\n",
    "        input_ids=np.array(train_df[TOKEN_IDS].values.tolist()),\n",
    "        attention_mask=np.array(train_df[ATTENTION_MASK].values.tolist()),\n",
    "        labels=train_df[TARGET].values\n",
    "    )\n",
    "\n",
    "\n",
    "def core(input_ids, attention_mask, labels):\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.int64).to(DEVICE)\n",
    "    attention_mask = torch.tensor(attention_mask, dtype=torch.int64).to(DEVICE)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float32).unsqueeze(1).to(DEVICE)\n",
    "\n",
    "    print(f'input_ids[:10]: {input_ids[:10]}')\n",
    "    print(f'attention_mask[:10]: {attention_mask[:10]}')\n",
    "\n",
    "    train_dataset = TensorDataset(input_ids, attention_mask, labels_tensor)\n",
    "    model = DisasterClassifier().to(DEVICE)\n",
    "    class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                         classes=np.array([0, 1]),\n",
    "                                         y=labels)\n",
    "    loop(\n",
    "        model=model,\n",
    "        train_loader=DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        ),\n",
    "        optimizer=optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-3),\n",
    "        criterion=nn.BCEWithLogitsLoss(\n",
    "            pos_weight=torch.tensor(class_weights[1] * 5,\n",
    "                                    dtype=torch.float32))\n",
    "    )\n",
    "\n",
    "\n",
    "def loop(model: DisasterClassifier, train_loader: DataLoader, optimizer: optim, criterion):\n",
    "    len_train_loader = len(train_loader)\n",
    "    for epoch in range(MAX_EPOCH):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_ids, attention_mask, labels in train_loader:\n",
    "            input_ids, attention_mask, labels = input_ids.to(DEVICE), attention_mask.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len_train_loader\n",
    "        print(f\"üîπ Epoch {epoch + 1}/{MAX_EPOCH}, \"\n",
    "              f\"Loss: {avg_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    save_model(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T00:18:51.190444Z",
     "start_time": "2025-02-24T00:18:51.185019Z"
    }
   },
   "id": "f3d60ba59b4fec93",
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids[:10]: tensor([[  101, 15046,  3114,  ...,     0,     0,     0],\n",
      "        [  101,  3224,  2543,  ...,     0,     0,     0],\n",
      "        [  101,  6319,  2356,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 10047,  2327,  ...,     0,     0,     0],\n",
      "        [  101,  2045,  5057,  ...,     0,     0,     0],\n",
      "        [  101, 10047,  4452,  ...,     0,     0,     0]], device='mps:0')\n",
      "attention_mask[:10]: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='mps:0')\n",
      "üîπ Epoch 1/7, Loss: 1.1974, LR: 0.000100\n",
      "üîπ Epoch 2/7, Loss: 0.7976, LR: 0.000100\n",
      "üîπ Epoch 3/7, Loss: 0.5258, LR: 0.000100\n",
      "üîπ Epoch 4/7, Loss: 0.3196, LR: 0.000100\n",
      "üîπ Epoch 5/7, Loss: 0.2501, LR: 0.000100\n",
      "üîπ Epoch 6/7, Loss: 0.1784, LR: 0.000100\n",
      "üîπ Epoch 7/7, Loss: 0.1546, LR: 0.000100\n"
     ]
    }
   ],
   "source": [
    "train_model(train_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T00:31:37.704199Z",
     "start_time": "2025-02-24T00:18:56.219455Z"
    }
   },
   "id": "b1717956d0703ba",
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = torch.tensor(\n",
    "            np.array(df[TOKEN_IDS].values.tolist()),\n",
    "            dtype=torch.int64)\n",
    "        self.attention_mask = torch.tensor(\n",
    "            np.array(df[ATTENTION_MASK].values.tolist()),\n",
    "            dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_mask[idx]\n",
    "\n",
    "\n",
    "def predict(test_df, threshold=0.5):\n",
    "    predications = []\n",
    "    model: DisasterClassifier = load_model()\n",
    "\n",
    "    test_dataset = TestDataset(test_df)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for input_ids, attention_mask in test_loader:\n",
    "            input_ids, attention_mask = input_ids.to(DEVICE), attention_mask.to(DEVICE)\n",
    "\n",
    "            output = model(input_ids, attention_mask)\n",
    "            probs = torch.sigmoid(output)\n",
    "            batch_predictions = (probs >= threshold).int()\n",
    "            predications.extend(batch_predictions.cpu().numpy().flatten().tolist())\n",
    "\n",
    "    print(\"Unique Predictions:\", np.unique(predications, return_counts=True))\n",
    "    save_result(test_df, predications)\n",
    "\n",
    "\n",
    "def load_model() -> DisasterClassifier:\n",
    "    checkpoint = torch.load(MODEL_SAVE_PATH)\n",
    "    model = DisasterClassifier().to(DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_result(test_df: pd.DataFrame, preds: list[np.ndarray]):\n",
    "    submission_df = pd.DataFrame({\"id\": test_df[\"id\"], \"target\": preds})\n",
    "    submission_df.to_csv(SUBMISSION_SAVE_PATH, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T00:35:04.315859Z",
     "start_time": "2025-02-24T00:35:04.311628Z"
    }
   },
   "id": "46272eec195dd8de",
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Predictions: (array([0, 1]), array([1864, 1399]))\n"
     ]
    }
   ],
   "source": [
    "test_df = test_df.fillna('null')\n",
    "predict(test_df, threshold=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T00:37:03.171692Z",
     "start_time": "2025-02-24T00:36:46.476706Z"
    }
   },
   "id": "e76ccf34469447d7",
   "execution_count": 95
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   id  target\n0   0       1\n1   2       1\n2   3       1\n3   9       1\n4  11       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(SUBMISSION_SAVE_PATH)\n",
    "submission.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T00:38:12.340040Z",
     "start_time": "2025-02-24T00:38:12.324327Z"
    }
   },
   "id": "62e6329cdb0fbd1a",
   "execution_count": 96
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
